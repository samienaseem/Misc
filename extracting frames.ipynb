{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"0kj0pdjITLEB"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":659,"status":"ok","timestamp":1724920939809,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"1v5B78PzcL6u","outputId":"e9286ea2-a9fa-452b-f1cf-fcd82e5969d0"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 0H4A5595.MOV         dist.pkl                               'Sample3 Reference1.tif'\n"," ADIC2D.ipynb         \u001b[0m\u001b[01;34mDSLRImages\u001b[0m/                            'Sample3 Reference2.tif'\n"," calibration.pkl      EDSR_x4.pt                             'Sample3 Reference.tif'\n"," \u001b[01;34mCameraCalibration\u001b[0m/   \u001b[01;34mimages\u001b[0m/                                 speckle_pattern.jpg\n"," cameraMatrix.pkl     keyPointExtraction.ipynb                Test.ipynb\n"," \u001b[01;34mcorrectedImage\u001b[0m/      \u001b[01;34moutput_images\u001b[0m/\n"," \u001b[01;34mDeepDIC\u001b[0m/            'Sample3-000 X0.00 Y0.00 N2 C0 R0.tif'\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724908162384,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"cjOBPzjQdsA8","outputId":"28dbfac3-5ed7-4792-a969-a42f42675530"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive'\n","/content/drive/MyDrive/DIC\n"]}],"source":["cd drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1724908162384,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"3F1wDV3Rdv7x","outputId":"a0b23583-8f21-45d6-dfa2-0db0c1ddd16f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'MyDrive/'\n","/content/drive/MyDrive/DIC\n"]}],"source":["cd MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1724908162384,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"-XfI6O8bd2ut","outputId":"0106de5c-a061-4685-a67f-bbfcc2a8e4bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'DIC'\n","/content/drive/MyDrive/DIC\n"]}],"source":["cd DIC"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6643,"status":"ok","timestamp":1724908169025,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"yx6IL_RKdrkf","outputId":"3fa60ad5-b1e1-415a-9cb7-fc5d403977d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pywavelets in /usr/local/lib/python3.10/dist-packages (1.7.0)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from pywavelets) (1.26.4)\n"]}],"source":["pip install pywavelets"]},{"cell_type":"code","source":["pip install Pillow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmjhHsUJXSqX","executionInfo":{"status":"ok","timestamp":1724922020864,"user_tz":-60,"elapsed":3078,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"}},"outputId":"d92cbe23-30da-4c1b-9e7d-d36db82e48ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFmkxb3ZeaLH"},"outputs":[],"source":["from skimage.restoration import estimate_sigma\n","import cv2\n","import pywt\n","import glob\n","import cv2\n","import numpy as np\n","from scipy.ndimage import gaussian_filter\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","from scipy.ndimage import median_filter\n","from skimage import exposure, filters\n","from skimage import img_as_float64"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":2840,"status":"error","timestamp":1724908171862,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"1GAyuHOZd3wm","outputId":"cb17a654-5483-4e1c-9ba1-e10190947e20"},"outputs":[{"output_type":"stream","name":"stdout","text":["correctedImage/DSLR_imagesDIC/Referencess.png\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-4648fc0be4c7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# img_double = img_gray.astype(np.float64)/ 255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mgrey_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'correctedImage/GreyImages/{img_path.split(\"/\")[-1]}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_gray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","\n","displacement_images_path = 'correctedImage/DSLR_imagesDIC/*.png'\n","grey_images=[]\n","displacement_image_paths = glob.glob(displacement_images_path)\n","# displacement_image_paths.append('correctedImage/corrected_refrence.png')\n","for i, img_path in enumerate(displacement_image_paths):\n","  print(img_path)\n","  img = cv2.imread(img_path)\n","  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  # img_double = img_gray.astype(np.float64)/ 255.0\n","  grey_images.append(img_gray)\n","  cv2.imwrite(f'correctedImage/GreyImages/{img_path.split(\"/\")[-1]}', img_gray)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLPztB4teGY-"},"outputs":[],"source":["grey_images_path = 'correctedImage/GreyImages/*.png'\n","grey_paths = glob.glob(grey_images_path)\n","speckle_image=[]\n","\n","sift = cv2.SIFT_create()\n","\n","for i, img_path in enumerate(grey_paths):\n","  image = cv2.imread(img_path)\n","  keypoints = sift.detect(image, None)\n","\n","  speckle_pattern_white = np.full_like(image, 255)\n","\n","  for kp in keypoints:\n","        x, y = int(kp.pt[0]), int(kp.pt[1])\n","        cv2.circle(speckle_pattern_white, (x, y), 2, (0), -1)\n","\n","    # Save the speckle pattern image\n","  speckle_image.append(speckle_pattern_white)\n","\n","  filename_with_ext = img_path.split(\"/\")[-1]\n","  filename, extension = filename_with_ext.split(\".\")\n","\n","  # Modify the filename by inserting an underscore before the number\n","  modified_filename = filename[:-1] + \"_\" + filename[-1]\n","\n","# Combine the modified filename with the extension\n","  new_filename = f\"{modified_filename}.{extension}\"\n","  cv2.imwrite(f'correctedImage/speckled/{new_filename}', speckle_pattern_white)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1724928646334,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"BIKtJAHwNkfj","outputId":"a75fa8e4-0b87-4d8c-df19-7085cec0192f"},"outputs":[{"output_type":"stream","name":"stdout","text":["correctedImage/DSLR_imagesDIC/Referencess.png\n","correctedImage/DSLR_imagesDIC/image1.png\n","correctedImage/DSLR_imagesDIC/image2.png\n","correctedImage/DSLR_imagesDIC/image3.png\n","correctedImage/DSLR_imagesDIC/image4.png\n","correctedImage/DSLR_imagesDIC/image5.png\n","correctedImage/DSLR_imagesDIC/image6.png\n","correctedImage/DSLR_imagesDIC/image7.png\n","correctedImage/DSLR_imagesDIC/image8.png\n","correctedImage/DSLR_imagesDIC/image9.png\n","correctedImage/DSLR_imagesDIC/image10.png\n","correctedImage/DSLR_imagesDIC/image11.png\n","correctedImage/DSLR_imagesDIC/image12.png\n","correctedImage/DSLR_imagesDIC/image13.png\n","correctedImage/DSLR_imagesDIC/image14.png\n","correctedImage/DSLR_imagesDIC/image15.png\n","correctedImage/DSLR_imagesDIC/image16.png\n","correctedImage/DSLR_imagesDIC/image17.png\n"]}],"source":["import glob\n","orig = 'correctedImage/DSLR_imagesDIC/*.png'\n","orig = glob.glob(orig)\n","image_paths = []\n","Filtered_images=[]\n","for i, img_path in enumerate((orig)):\n","    print(img_path)\n","    # img = cv2.imread(img_path)\n","    # img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    # # img_double = img_gray.astype(np.float64)/ 255.0\n","    image_paths.append(img_path)\n","    # Filtered_images.append(img_gray\n","    # cv2.imwrite(f'correctedImage/Sample1Processed/{img_path.split(\"/\")[-1]}', img_gray)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EH76brbvPzH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opu1gY0nysXz"},"outputs":[],"source":["def estimate_noise(img):\n","  noise_level_sigma = estimate_sigma(img, average_sigmas=True)\n","  return noise_level_sigma"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMHnmmwdzO1I"},"outputs":[],"source":["def adaptive_preprocessing(img):\n","    \"\"\"\n","    Perform adaptive preprocessing: Gaussian and median filtering based on estimated noise.\n","    \"\"\"\n","\n","    img = img_as_float64(image)\n","\n","\n","    noise_level = estimate_noise(img)\n","    print(f\"Noise level: {noise_level}\")\n","\n","\n","    # Convert image to PyTorch tensor\n","    img_tensor = torch.from_numpy(img).float().unsqueeze(0).unsqueeze(0).cuda()\n","\n","    # Apply adaptive Gaussian filter\n","    gaussian_filtered = adaptive_gaussian_filter(img_tensor, noise_level).cpu().numpy().squeeze()\n","    noise_level = estimate_noise(gaussian_filtered)\n","    print(f\"Noise level after gaussian Filter: {noise_level}\")\n","\n","    # # Apply adaptive median filter\n","    # median_filtered = adaptive_median_filter(gaussian_filtered, noise_level)\n","\n","    #Apply Enhaced CLAHE\n","    enhanced_clahe = iterative_clahe(gaussian_filtered,iterations=5)\n","\n","\n","    noise_level_2 = estimate_noise(enhanced_clahe)\n","    print(f\"Noise level after enhaced CLAHE: {noise_level_2}\")\n","\n","    enhanced_clahe_tensor = torch.from_numpy(enhanced_clahe).float().unsqueeze(0).unsqueeze(0).cuda()\n","\n","    gaussian_filtered_2 = adaptive_gaussian_filter(enhanced_clahe_tensor, noise_level_2).cpu().numpy().squeeze()\n","    noise_level_3 = estimate_noise(gaussian_filtered_2)\n","    print(f\"Noise level after gaussian Filter x 2: {noise_level_3}\")\n","\n","    return gaussian_filtered_2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vs-DOwwXvdqh"},"outputs":[],"source":["\n","def gaussian_kernel(size, sigma, device):\n","    x = torch.arange(size, dtype=torch.float32, device=device) - (size - 1) / 2\n","    x = x.repeat(size, 1)\n","    y = x.t()\n","    kernel = torch.exp(-(x**2 + y**2) / (2 * sigma**2))\n","    kernel /= kernel.sum()\n","    return kernel\n","\n","def adaptive_gaussian_filter(image_tensor, estimated_noise):\n","    \"\"\"Apply adaptive Gaussian filter based on estimated noise.\"\"\"\n","    # Ensure the input is 2D or 3D\n","    if image_tensor.dim() > 4:\n","        image_tensor = image_tensor.squeeze()\n","\n","    # If the tensor is 2D, add batch and channel dimensions\n","    if image_tensor.dim() == 2:\n","        image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)\n","    # If the tensor is 3D (assuming it's a single-channel image with batch dimension)\n","    elif image_tensor.dim() == 3:\n","        image_tensor = image_tensor.unsqueeze(1)\n","\n","    sigma = max(0.3, min(2.0, estimated_noise / 10))\n","    kernel_size = int(2 * round(3 * sigma) + 1)\n","\n","    # Create Gaussian kernel\n","    kernel = gaussian_kernel(kernel_size, sigma, image_tensor.device)\n","\n","    #Uniform Kernel\n","    # kernel = torch.ones(kernel_size, kernel_size, device=image_tensor.device)\n","    # kernel = kernel / kernel.sum()\n","\n","    padding = kernel_size // 2\n","    filtered = F.conv2d(image_tensor,\n","                        kernel.unsqueeze(0).unsqueeze(0),\n","                        padding=padding)\n","\n","    return filtered.squeeze()"]},{"cell_type":"code","source":["def adaptive_median_filter(img, noise_level):\n","    initial_size = 3\n","    max_size = int(noise_level * 5)\n","    print(f\"Noise level {noise_level} and max size = {max_size}\")\n","\n","    img_filtered = median_filter(img, size=initial_size)\n","\n","    for y in range(img.shape[0]):\n","        for x in range(img.shape[1]):\n","            window_size = initial_size\n","            while window_size <= max_size:\n","                local_window = img[max(0, y-window_size//2):min(img.shape[0], y+window_size//2+1),\n","                                   max(0, x-window_size//2):min(img.shape[1], x+window_size//2+1)]\n","                local_median = np.median(local_window)\n","                if abs(img[y, x] - local_median) < noise_level:\n","                    img_filtered[y, x] = local_median\n","                    break\n","                window_size += 2\n","\n","    return img_filtered"],"metadata":{"id":"O0x-Dw2kRbiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def enhanced_adaptive_clahe(image, noise_level, base_clip_limit=2.0, base_tile_size=8 ):\n","#     \"\"\"\n","#     Apply further optimized Adaptive CLAHE.\n","\n","#     :param image: Input image (2D numpy array)\n","#     :param base_clip_limit: Base clipping limit for CLAHE\n","#     :param base_tile_size: Base tile size for CLAHE\n","#     :return: Optimized CLAHE-enhanced image\n","#     \"\"\"\n","#     # Convert image to 8-bit if necessary\n","#     if image.dtype != np.uint8:\n","#         image = exposure.rescale_intensity(image, out_range=(0, 255)).astype(np.uint8)\n","\n","#     # # Multi-scale noise estimation (example: using Gaussian blur)\n","#     # noise_level = estimate_sigma(image, average_sigmas=True)\n","\n","#     # Edge density estimation (using Sobel filter or other methods)\n","#     edges = filters.sobel(image)\n","#     edge_density = np.mean(edges)\n","\n","#     # Dynamic adjustment of clip limit and tile size\n","#     adjusted_clip_limit = base_clip_limit * (1 + edge_density) / (1 + noise_level)\n","#     adjusted_clip_limit = np.clip(adjusted_clip_limit, 0.8, 4.0)\n","\n","#     image_size = min(image.shape)\n","#     size_factor = image_size / 512\n","#     adjusted_tile_size = int(base_tile_size * size_factor * (1 / (1 + edge_density)))\n","#     adjusted_tile_size = max(4, min(adjusted_tile_size, 16))\n","\n","#     # Apply CLAHE\n","#     clahe = cv2.createCLAHE(clipLimit=adjusted_clip_limit,\n","#                             tileGridSize=(adjusted_tile_size, adjusted_tile_size))\n","#     enhanced_image = clahe.apply(image)\n","\n","#     print(f\"Enhanced CLAHE parameters - Clip Limit: {adjusted_clip_limit:.2f}, Tile Size: {adjusted_tile_size}\")\n","\n","#     return enhanced_image\n","\n","\n","def iterative_clahe(image, base_clip_limit=2.0, base_tile_size=8, iterations=5):\n","    \"\"\"\n","    Apply iterative CLAHE with adjustments based on the previous result.\n","\n","    :param image: Input image (2D numpy array)\n","    :param base_clip_limit: Base clipping limit for CLAHE\n","    :param base_tile_size: Base tile size for CLAHE\n","    :param iterations: Number of iterations to perform\n","    :return: Iteratively enhanced image\n","    \"\"\"\n","\n","    # Convert image to 8-bit if necessary\n","    if image.dtype != np.uint8:\n","        image = exposure.rescale_intensity(image, out_range=(0, 255)).astype(np.uint8)\n","\n","    current_image = image.copy()\n","\n","    for i in range(iterations):\n","        noise_level = estimate_sigma(current_image, average_sigmas=True)\n","        print(f\"Iteration {i+1}: Noise level = {noise_level}\")\n","        edges = filters.sobel(current_image)\n","        edge_density = np.mean(edges)\n","\n","        adjusted_clip_limit = base_clip_limit * (1 + edge_density) / (1 + noise_level)\n","        adjusted_clip_limit = np.clip(adjusted_clip_limit, 0.8, 4.0)\n","\n","        image_size = min(current_image.shape)\n","        size_factor = image_size / 512\n","        adjusted_tile_size = int(base_tile_size * size_factor * (1 / (1 + edge_density)))\n","        adjusted_tile_size = max(4, min(adjusted_tile_size, 16))\n","\n","        clahe = cv2.createCLAHE(clipLimit=adjusted_clip_limit,\n","                                tileGridSize=(adjusted_tile_size, adjusted_tile_size))\n","        current_image = clahe.apply(current_image)\n","\n","        print(f\"Iteration {i+1}: Clip Limit = {adjusted_clip_limit:.2f}, Tile Size = {adjusted_tile_size}\")\n","\n","    return current_image\n","\n"],"metadata":{"id":"fCQ8WN7KykWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1XwUepIBoUY7r7ezMI6qUyZJVyqkAmWsM"},"executionInfo":{"elapsed":50106,"status":"ok","timestamp":1724928711370,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"},"user_tz":-60},"id":"r6fLgck3fYFi","outputId":"c8748a97-7e4d-4437-9b95-10078e2edb90"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","# Process all images again\n","processed_images = []\n","sigma_estimates = []\n","gaussian_sigmas = []\n","\n","for path in image_paths:\n","    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","    print(f\"type = {image.dtype}\")\n","    filtered_image= adaptive_preprocessing(image)\n","    processed_images.append(filtered_image)\n","# Display the original and filtered images for comparison\n","\n","\n","for i, (original_path, processed_image) in enumerate(zip(image_paths, processed_images)):\n","    original_image = cv2.imread(original_path, cv2.IMREAD_GRAYSCALE)\n","    print(f\"Image {i+1}: shape={processed_image.shape}, dtype={processed_image.dtype}, min={processed_image.min()}, max={processed_image.max()}, path={original_path}\")\n","    plt.figure(figsize=(10, 10))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(original_image, cmap='gray')\n","    plt.title('Original Image')\n","    plt.axis('off')\n","\n","    #img = to_torch(img)\n","    # Filtered_images.append(filtered_img);\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(processed_image, cmap='gray')\n","    plt.title('Processed Image')\n","    plt.axis('off')\n","    plt.show()\n","    print(f\"i= {i}\")\n","# Return the sigma estimates and applied Gaussian sigmas for further analysis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ph3T5cwJzLKY"},"outputs":[],"source":["for i, img in enumerate(processed_images):\n","  cv2.imwrite(f'correctedImage/Sample1Processed/SampleImage_{i}.tif', img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3iS2jG6c-x6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724929366692,"user_tz":-60,"elapsed":1032,"user":{"displayName":"Saquib Naseem","userId":"16339182378247332285"}},"outputId":"cff048de-ea0f-4398-e001-9397fb258793"},"outputs":[{"output_type":"stream","name":"stdout","text":["All images have been saved successfully.\n"]}],"source":["import cv2\n","import os\n","\n","# Directory path where the images will be saved\n","save_directory = 'correctedImage/DSLR1Processed/'\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(save_directory, exist_ok=True)\n","\n","# Loop through the processed images and save them\n","for i, img in enumerate(processed_images):\n","    # Ensure the image is in the correct format (uint8 or uint16)\n","    if img.dtype != np.uint8 and img.dtype != np.uint16:\n","        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)  # Normalize and convert to uint8\n","\n","    # Save the image with a unique filename\n","    cv2.imwrite(os.path.join(save_directory, f'SampleImage_{i}.tif'), img)\n","\n","print(\"All images have been saved successfully.\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"eJEe53JSerOd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm","mount_file_id":"1Z-jt1VNN8baNz4TjGjEzgTzS9cOduJGM","authorship_tag":"ABX9TyNJTeFIRQQNc2I3ewzbeqtl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}